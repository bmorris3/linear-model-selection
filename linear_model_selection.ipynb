{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functional-sender",
   "metadata": {},
   "source": [
    "# Model comparison with PyMC3\n",
    "### Brett Morris\n",
    "\n",
    "EEG Meeting, 27 April 2021\n",
    "\n",
    "# Setup\n",
    "Install the necessary packages with: \n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge pymc3\n",
    "pip install pymc3-ext corner arviz\n",
    "```\n",
    "\n",
    "**Troubleshooting**: Getting PyMC3 up and running can be complicated, see these pages for the most up-to-date notes on how to install on: [Mac](https://github.com/pymc-devs/pymc3/wiki/Installation-Guide-(MacOS)), [Linux](https://github.com/pymc-devs/pymc3/wiki/Installation-Guide-(Linux)), [Windows](https://github.com/pymc-devs/pymc3/wiki/Installation-Guide-(Windows)). If those instructions fail, you can usually get quick help on this [PyMC discourse page](https://discourse.pymc.io).\n",
    "\n",
    "# Background\n",
    "\n",
    "Often in observational astronomy, we have several plausible physical models which could fit the data, and we wish to identify which model best fits the data with the fewest free parameters. This is often the case with CHEOPS observations, where the observations can often be modeled as a linear combination of several *basis vectors*, or vectors of measurements like the telescope temperature or time, with the same length as your time series measurements $y$. **See also** [this repo](https://github.com/bmorris3/photometry_linalg) which I presented at a previous group meeting on photometric reductions with linear regression.\n",
    "\n",
    "### Linear algebra recap\n",
    "\n",
    "In a linear observation model, we often suppose that our observations $y(t)$ can be approximated with a linear combination of the $N$ basis vectors $a_i(t)$ like so: \n",
    "\n",
    "$$ y(t) = w_0 a_0(t) + w_1 a_1(t) + ... + w_N a_N(t)$$\n",
    "\n",
    "where the $w_i$ coefficients are the *weights* on each basis vector which minimize the residuals between the observations and the linear combination of basis vectors. This is a least-squares problem, which can be phrased like so: the design matrix $X$,\n",
    "\n",
    "\\begin{equation}\n",
    "{\\bf X}  = \\begin{bmatrix}\n",
    "a_0(t_0) & a_1(t_0) & ... & a_N(t_0)\\\\\n",
    "a_0(t_1) & a_1(t_1) & ... & a_N(t_1)\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "a_0(t_n) & a_1(t_n) & ... & a_N(t_n)\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "is the concatenation of each basis column vector $a_i$. We want to solve for the weights \n",
    "\\begin{equation}\n",
    "{\\bf \\hat{\\beta}} = \\left[ w_0 ~ w_1 ~ ... ~  w_N \\right]\n",
    "\\end{equation}\n",
    "which minimize\n",
    "\n",
    "$$ \\arg \\min_\\beta || {\\bf y - X \\hat{\\beta}} ||. $$\n",
    "\n",
    "The best-fit estimators $\\hat{\\beta}$ can be solved with some matrix math (see [Wikipedia](https://en.wikipedia.org/wiki/Ordinary_least_squares) for more):\n",
    "\n",
    "$${\\displaystyle {\\hat {\\boldsymbol {\\beta }}}=(\\mathbf {X} ^{\\rm {T}} N^{-1} \\mathbf {X} )^{-1}\\mathbf {X} ^{\\rm {T}} N^{-1} \\mathbf {y}, }$$\n",
    "\n",
    "where $\\bf N = \\sigma_y^2 {\\bf I}$ is the observational uncertainty on each measurement $\\sigma_y$, multiplied by the identity matrix $\\bf I$.\n",
    "\n",
    "### Why use PyMC3?\n",
    "\n",
    "> PyMC3 is a new, open-source [pure Python] framework with an intuitive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the **No-U-Turn Sampler** (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). This class of samplers works well on **high dimensional and complex posterior distributions** and allows many complex models to be fit without specialized knowledge about fitting algorithms.\n",
    "\n",
    "> While most of PyMC3’s user-facing features are written in pure Python, it leverages Theano (Bergstra et al., 2010) to transparently **transcode models to C** and compile them to machine code, thereby boosting performance.\n",
    "\n",
    "Source: [PyMC3 docs](https://docs.pymc.io/notebooks/getting_started.html), see also the [Example Gallery](https://docs.pymc.io/nb_examples/index.html), and [this manuscript on HMC methods](https://arxiv.org/abs/1701.02434)\n",
    "\n",
    "\n",
    "# Example with [PyMC3](https://docs.pymc.io)\n",
    "Next let's import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "import pymc3_ext as pmx\n",
    "import arviz as az\n",
    "from corner import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-development",
   "metadata": {},
   "source": [
    "Generate an example dataset with three components: linear, quadratic, and sinusoidal trends. Add random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "n = 1000\n",
    "x = np.linspace(-6, 6, n)\n",
    "\n",
    "truths = [\n",
    "    2,    # DC offset of the whole curve\n",
    "    3,    # linear trend weight\n",
    "    0.2,  # quadratic trend weight\n",
    "    2     # sinusoidal weight\n",
    "]\n",
    "\n",
    "true_linear = truths[0] + truths[1] * x\n",
    "true_period = 0.8\n",
    "true_quadratic = truths[2] * x**2\n",
    "true_sinusoid = truths[3] * np.sin(2 * np.pi * x / true_period)\n",
    "\n",
    "uncertainties = 0.5\n",
    "noise = np.random.normal(scale=0.5, size=n)\n",
    "\n",
    "y = (\n",
    "    true_linear + \n",
    "    true_quadratic + \n",
    "    true_sinusoid \n",
    ") + noise\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, y, 'k.', label='observations')\n",
    "plt.plot(x, true_linear, label='linear')\n",
    "plt.plot(x, true_sinusoid, label='sin')\n",
    "plt.plot(x, true_quadratic, label='quad')\n",
    "plt.legend()\n",
    "plt.gca().set(xlabel='$x$', ylabel='$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-function",
   "metadata": {},
   "source": [
    "Create a list of (unscaled) basis vectors, which we will use to solve for the linear regression coefficients (weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_vectors = [\n",
    "    np.ones_like(x), # DC offset\n",
    "    x,               # linear trend\n",
    "    x**2,            # quadratic trend\n",
    "    np.sin(2 * np.pi * x / true_period), # sinusoid\n",
    "     -(np.abs(x) < 2).astype(int) # extra, unnecessary vector    \n",
    "]\n",
    "\n",
    "basis_vector_names = [\n",
    "    \"unit\", \n",
    "    \"$x$\", \n",
    "    \"$x^2$\", \n",
    "    \"$\\sin$\",\n",
    "    \"box\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-stocks",
   "metadata": {},
   "source": [
    "Loop through basis vectors, adding one at a time, and evaluating the best-fit linear regression coefficients for each combination of basis vectors. Save results to a dictionary called ``inference_data_dict``, and plot the *maximum a posteriori* (MAP) solution in red over the data in black:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_basis_vectors = len(basis_vectors)\n",
    "\n",
    "inference_data_dict = {}\n",
    "\n",
    "for n_basis_vectors in range(1, total_basis_vectors + 1):\n",
    "    \n",
    "    model_name = f\"n={n_basis_vectors}\"\n",
    "    \n",
    "    with pm.Model(name=model_name) as pymc3_model: \n",
    "    \n",
    "        beta = pm.Uniform(\"beta\", lower=-10, upper=10, shape=n_basis_vectors)\n",
    "    \n",
    "        linear_model = pm.math.dot(beta, basis_vectors[:n_basis_vectors])\n",
    "    \n",
    "        pm.Normal(\"obs\", mu=linear_model, sigma=uncertainties, observed=y)\n",
    "        \n",
    "        map_soln = pm.find_MAP()\n",
    "\n",
    "        plt.plot(x, y, '.k')\n",
    "        plt.plot(x, pmx.eval_in_model(linear_model, map_soln), 'r')\n",
    "        plt.title(\"Basis vectors: \" + \", \".join(basis_vector_names[:n_basis_vectors]))\n",
    "        plt.gca().set(xlabel='$x$', ylabel='$y$')\n",
    "        plt.show()\n",
    "        \n",
    "        inference_data = pm.sample(\n",
    "            draws=1000,\n",
    "            tune=1000,\n",
    "            compute_convergence_checks=False,\n",
    "            return_inferencedata=True,\n",
    "            target_accept=0.9,\n",
    "            cores=2\n",
    "        )\n",
    "        \n",
    "        inference_data_dict[model_name] = inference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-tunisia",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "Now that we've solved for the best-fit weights on each basis vector for all five basis vectors. Which combination of basis vectors best reproduces the observations? **Which model do we choose?**\n",
    "\n",
    "One technique for measuring the relative confidence that a set of models fits the data is the [Leave-One-Out cross validation statistic](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation). In its simplest form, the LOO will do the following: \n",
    "\n",
    "1. Divide the observations into test and training groups of length $1$ and $n-1$\n",
    "2. Fit the model on the $n-1$ training observations, predict measurement at the \"missing\" data point\n",
    "3. Compare the prediction at the \"missing\" point to the actual observation\n",
    "4. Sum the squares of these residuals and normalize by the number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_loo = az.compare(\n",
    "    inference_data_dict, \n",
    "    ic='loo'\n",
    ")\n",
    "\n",
    "comparison_loo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-picture",
   "metadata": {},
   "source": [
    "For illustrative purposes, we compute the leave-one-out cross validation statistic in the [style of the wikipedia entry](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation) for each model explicitly here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the number of basis vectors:\n",
    "for n_basis_vectors in range(1, len(basis_vectors) + 1):\n",
    "    \n",
    "    # Construct a design matrix out of the first N basis vectors\n",
    "    design_matrix = np.vstack(basis_vectors[:n_basis_vectors]).T\n",
    "\n",
    "    # Compute the LOO by leaving out one data point, refitting the\n",
    "    # model, and subtracting the best-fit expectation from the \n",
    "    # actual observation at the missing data point:\n",
    "    loo_statistic = 0\n",
    "    for i in range(len(x)):\n",
    "        indices = np.r_[0:i-1, i+1:len(x)]   # indexing trick to skip `i`th index\n",
    "        x_in = x[indices]\n",
    "        y_in = y[indices]\n",
    "        design_matrix_in = design_matrix[indices, :]\n",
    "        \n",
    "        # Compute the least squares regression:\n",
    "        betas = np.linalg.lstsq(design_matrix_in, y_in, rcond=-1)[0]\n",
    "        \n",
    "        # Compute the value of the model at the missing time\n",
    "        y_out = design_matrix[i, :] @ betas\n",
    "\n",
    "        # Compute the difference between expected and observed at\n",
    "        # the missing data point:\n",
    "        loo_statistic += (y[i] - y_out)**2\n",
    "        \n",
    "    # Normalize over the length of the dataset:\n",
    "    loo_statistic /= len(x)\n",
    "    print(f\"LOO for n={n_basis_vectors}: {loo_statistic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-packing",
   "metadata": {},
   "source": [
    "Note that the LOO is smallest for the $n=\\{4,5\\}$ models, and in that case, you may want to select the model with the fewest free parameters, in this case $n=4$.\n",
    "\n",
    "We can also compute other information criteria, such as the \"widely applicable information criterion\" (read [all of the details here](http://www.stat.columbia.edu/~gelman/research/published/waic_understand3.pdf)) with arviz: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_waic = az.compare(\n",
    "    inference_data_dict, \n",
    "    ic='waic'\n",
    ")\n",
    "\n",
    "comparison_waic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-tunisia",
   "metadata": {},
   "source": [
    "If you compare the WAIC stats to the LOO stats above, you'll see that they agree closely.\n",
    "\n",
    "Finally, let's make a corner plot of the $n=4$ model to see how well the best-fit predictions for ${\\bf \\hat{\\beta}}$ compare with the true (input) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the posteriors to a flat chain for plotting a corner plot:\n",
    "post = inference_data_dict['n=4'].posterior['n=4_beta'].values\n",
    "flatchain = post.reshape((post.shape[0] * post.shape[1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(flatchain, labels=basis_vector_names, truths=truths);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-knowing",
   "metadata": {},
   "source": [
    "The agreement is quite good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-surrey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
